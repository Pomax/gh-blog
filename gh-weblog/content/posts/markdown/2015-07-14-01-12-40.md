Look around you. Look at the room you're in, look out the window, simply look at things and now imagine "what would this look like if I took a photograph". I'll spoil the answer: it'll basically look nothing like what you're seeing. The reasons for this are varied, but it basically boils down to "we settled for good enough, at a time when we didn't know what good enough meant".

## How many colours are there?

Short answer: that question makes no sense.

Longer answer: this depends on the definition of "colour" you go with, and before you object, there isn't just one definition so this is *super* important. Physically speaking (as in, based on physics) "a colour" isn't really a thing: it's just a word that is used —grounded in a specific cultural understanding— to describe a collection of frequencies and intensities of light.

Take the basic English colours "blue" and "red": these are not really one, exact, frequency, at one, exact, intensity each. They're clusters of frequencies at clusters of intensities. And while they may seems a basic colour, not all cultures agree on that. for instance, Hungarian has [two different words](https://en.wikipedia.org/wiki/Hungarian_language#Two_words_for_.22red.22)  for different, non-overlapping colours both of which are just "red" in English, and even though Japanese has a word for blue, <a href="https://en.wikipedia.org/wiki/Ao_(color)">青 ("ao")</a> it doesn't quite match the same *kinds* of blue that an English speaker might think of (the Japanese term includes [teals](https://en.wikipedia.org/wiki/Teal), for instance,  which many English speakers will identify as a color distinct from blue, in between blue and green). Colours are words, not actual frequencies in the [visible light part of the electro-magnetic spectrum](https://en.wikipedia.org/wiki/Light#Electromagnetic_spectrum_and_visible_light) that we know from physics.

So, asking "how many colours" there are isn't going to get us anywhere, so let's look at that visible light part of the electro-magnetic spectrum instead:

## How wide is the "physical colours" spectrum?

Now we're thinking with science: there are a (perhaps surprisingly) countable number of frequencies in the visible light spectrum, and an (again, perhaps surprisingly) countable number of intensities of light. However, the reason the frequencies and intensities are countable is due to quantum mechanics, and for any practical purpose we're not going to be able to count them. However, we can certainly plot them in a graph, to get a bit of insight into what we might be able to do if we want to capture light accurately.

I'm going to go with the [CIE 1931 representation of visible light](https://en.wikipedia.org/wiki/CIE_1931_color_space), which is a model based on (admittedly, old) research on how the human eye deals with colour, and it plots "all" the colours we can see, inside a "horseshoe" (seriously, that's what the world has decided to call this outline) that represents the EM spectral frequencies. Along the outer curve are all the "physically real" colours, i.e. distinct frequencies of light that we humans experience as "having a colour" (extend the curve on the lower left and you get into, to humans, invisible ultra-violet light, and extend it on the right and you get into, again invisible to humans, infra-red light), and on the inside are all the colours we can get by blending one or more of these frequencies.

<img src="https://upload.wikimedia.org/wikipedia/commons/5/5f/CIE-1931_diagram_in_LAB_space.svg" style="width:500px">

So something fun to note: there are "colours" in our language(s) that are not "frequencies of light" in physics. These are colours of which people will gleefully say "they are not colours" and then explain the difference between a word and an EM frequency, so, let's go:

- The "colour" black is the absence of light. No surprise there. It's a colour, as word, but not a colour, as frequency of light. It's technically every frequency, in any mixture, at intensity zero, which is the colour theory equivalent of a division by zero, and great to debate over drinks, but for science, meaningless.

- The "colour" white is an even mixture of "enough frequencies". It's not just one colour, it's "lots of colours, mixed in the right way".

Technically, in order for something to look white to the human eye, you just need an even mixture of light are a frequency of 420–440 nm ("violet blue"), 534–555 nm ("green") and 564–580 nm ("orange"), because [those are the peak frequencies for the light-sensing molecules in our retinae](https://en.wikipedia.org/wiki/Cone_cell#Types) (our eyes are not digital displays, and they don't work best for blue, green, and red: those are just the colours digital displays need to use because those describe the *extremes* of what our eyes can see. Although not really: more on that in a bit). However, you can mix many colours in many ways and get a colour that to the human eye looks "neutral", and as long as the intensity is high enough, the human eye will say that it "is white".

So black and white are generally understood but:

- We also see that the colour "purple" isn't a physical colour: all the colours from blueish purple all the way to red, are mixtures. Purple doesn't exist as "single" frequency in the real world. When you think you're seeing purple, your eyes are seeing a mixture of (predominantly) reds and blues, and while your eyes *are* registering the purple as two different colours at the receptor level, your brain —which is responsible for interpreting the signals— simply assigns the combination of signals it sees a single category, in the same way it will make you think you're seeing "green" regardless of whether you're seeing a pure green, or a mixture of blue and yellow. That's very useful for you, but not very useful for digitally encoding colours from real life.

## So, given "all physical colours", how bad is digital colour?

Let's just get this out of the way: **all the digital colours you are used to from daily interaction are *absolutely terrible* at representing physical colours**.

The problem here is historical: when we first went from analog to digital, technology was expensive, digital circuitry was really still in its infancy, and computation was slow. So, a minimal solution for "getting colour" was chosen: we can't reproduce all frequencies in all intensities, so lets pick a minimum number of colours that we *can* generate at varying intensity, in such a way that they can be mixed to form a reasonable number of colours.

And so we did.

Initially we had [monochrome displays](https://en.wikipedia.org/wiki/Monochrome_monitor) and printers, then we slowly increased the palettes: [from 16 "common" colours to 256 colours (8 bit colour), and then when the technology allowed for it, 8 bits for three specific primary colours each that we could mix to get about 16 million colours: 24 bit colour (and, if we include the transparency of a colour, 32 bit colour)](https://en.wikipedia.org/wiki/Color_depth).

In 1996, the ["Standard RGB"](https://en.wikipedia.org/wiki/SRGB) model (known today as sRGB) was proposed by HP and Microsoft, as a 24 bit colour scheme that would work across monitors and printers (so you could print "what you saw on the screen"), and it used a Red, Green and Blue as primary, which allowed it to express quite a lot of different colours:

<img src="https://upload.wikimedia.org/wikipedia/commons/6/60/Cie_Chart_with_sRGB_gamut_by_spigget.png" style="width:500px">

While this worked, the triangle of possible colours, called a ["Gamut"](https://en.wikipedia.org/wiki/Gamut), was fairly restricted, and in 1998 Adobe released their own, competing model called [AdobeRGB](https://en.wikipedia.org/wiki/Adobe_RGB_color_space) (also known as aRGB, to contrast sRGB) with colours that lined up better with the CMYK colours in industrial printers, and because it captures more physical colours, kind of "won": aRGB is likely the model that your monitor, camera, cellphone, etc, uses. The main difference is that it uses a different primary "green", and thus captures more colours:

<img src="https://upload.wikimedia.org/wikipedia/commons/0/0f/CIExy1931_AdobeRGB_vs_sRGB.png" style="width:500px">

Better as this is, it's still essentially worthless if you're interested in taking "colours you see in real life, and getting them digitally encoded" because while aRGB covers more real world colours, it still fails at representing about **half of all colours**.

(And to bring matters into immediate focus: even though aRGB is wider, virtually all regular computer monitors only do sRGB.)

## What if we made a big triangle, e.g. with primaries at 380nm / 515nm / 700nm?

Great idea! While we'd still have a triangular gamut, we'd capture so much more! In fact, if you've heard of [Ultra High Definition Television](https://en.wikipedia.org/wiki/Ultra-high-definition_television), this is exactly what they've done with the [Rec2020](https://en.wikipedia.org/wiki/Rec._2020) colour model, which is intended for "4K" devices (which aren't actually 4k; they're 3840 by 2160 points):

<img src="https://upload.wikimedia.org/wikipedia/commons/b/b6/CIExy1931_Rec_2020.svg" style="width:500px">

A great example of a company that's succeeded with this is [Blackmagic Design](https://www.blackmagicdesign.com/products/cinemacameras), who make digital film equipment, and have carved a spectacular niche for themselves in the 4k pro-and-consumer market. Unfortunately, while it might seem like digital film equipment could have a ripple effect on digital stills equipment, that hasn't really been the case.

Another thing is that if we wanted to be able to *print* these colours, we'd have to come up with new printer inks. For television, this is not a concern: you wouldn't print movies (what would that even mean?) but for photography, the notion of printing is still very much extant, so... we *could* reformulate the printer inks, but if we're going to switch up the printer inks, we can probably do better.

## "Better", like virtual colours?

"Wait, what? Virtual colours?" And why not: we can *encode* colours using entirely virtual primaries, so that the triangle they span completely covers the real world colour spectrum. Sure, there are combinations of those primaries that have no meaning, but whatever: we just don't store values that don't make sense, and done. In fact, the [ProPhoto model](https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space) does this:

<img src="https://upload.wikimedia.org/wikipedia/commons/e/eb/CIExy1931_ProPhoto.svg" style="width:500px">

Most of the real world colours fall inside its gamut, with primaries picked such that there aren't *too* many meaningless combinations possible.

Unfortunately, of course, this colour space cannot be printed using primary inks because they'd need to be in colours that, rather crucially, **cannot exist in our universe**. Furthermore, we also can't *record* colours in terms of these primaries, since we'd have to create sensors that act as primaries, and they'd have to trigger based on negative frequencies. It... gets weird. So PhotoPro RGB is rather useful as digital storage format, but *only* as digital storage format: showing what it looks like is always going to lead to problems.

## What about more primaries?

Now you've got your thinking-cap on: why are we still sampling the real world with only three sensors? Sure, the human eye works that way, but humans have weird conditions (some people can see four distinct colours, not three, we call them [tetrachromats](https://en.wikipedia.org/wiki/Tetrachromacy#Human_tetrachromats); some people can't even see three: we call them [color blind](https://en.wikipedia.org/wiki/Color_blindness) but they can see light just fine, and simply see different ranges of colour, in which some colours that trichomats see as distinct are seen as the same. No blindness there).

Why don't we just record colours using more sensors? We could use one red (680ish), three different greens (545,518,505), a cyan (490) and a blue (say 460ish) and now we'd be able to at least *capture* virtually all the colours that exist in the real world.

<img src="/images/cie-six-point.jpg" style="width:500px">

Does this make sense? From a computer science point of view: yes, totally. After all, this is our base data that we probably want to do things to, like lighten/darken, hue shift, blend, etc and only encoding the values that the human eye can see, rather than all the colours we actually have in real life, means that lots of colour operations are going to do "the wrong thing" for colours that live at the edges of the Gamut. Remove those edges, and things get much better.

We can even efficiently store the numbers, because even though we have six primaries, we only ever need three primaries to form any colour, and then the other primaries can simply contribute a neutral intensity:

<img src="/images/cie-six-point-q.jpg" style="width:500px">

We have six primaries, so we have four quadrants, so we can neatly prefix any colour with two bits to indicate which quadrant it falls in, and then the intensities for the three primaries involved in that quadrant: a "green" would be "00" + {three intensities}, for instance.

## But how would recording this even work? Six sensors?

To be fair, if you think six sensors to capture colour is a problem, realize that [using three is also a problem](https://en.wikipedia.org/wiki/Color_filter_array#List_of_color_filter_arrays): we don't really capture the colours of things, we capture [single colour channels](https://en.wikipedia.org/wiki/Image_sensor#Color_separation) of things, and then assume that we can reasonably accurately construct points *between* our sensor points by mixing the colours around it. As long as our points are small enough, that works, but it can leads to interesting problems such as [demosaicing](https://en.wikipedia.org/wiki/Demosaicing) and dealing with [Moiré patterns](https://en.wikipedia.org/wiki/Moir%C3%A9_pattern).

Having six, rather than three, colours basically makes this problem a little harder, although we can mitigate this by having a point layout that we can perform reasonable post-processing with. It's a problem, but not more so than it already is.

## What about intensities?

The human eye is *really good* at distinguishing light intensity values, although only given some ambient lighting value. The human eye is a *contrast* detector, and is really good at that job, with [a huge range of intensity detection](https://en.wikipedia.org/wiki/Lux#Illuminance), ranging from contrasts at ambient values of 0.01 lux for a moonlight night to a 25,000 lux on a sunny day, to being able to distinguish features at even higher lux values (pretty much in direct sunlight). Basically, the human eye is much better than a digital camera at recording light intensities. So digital cameras make do:

Modern digital cameras will record at 14ish bits worth of intensity, which gives 16384 distinct base intensity values per pixel, and so for any image pixel that is built up of three primaries, we can actually get three times that range, although most of the range will have bias for some specific primary colour:

- RGB (0,0,0) is pure black, without bias
- (1,0,0) and (0,1,0) and (0,0,1) are "lighter than black" but "fainter than the faintest grey"
- (1,1,0) and (1,0,1) and (0,1,1) are even "lighter than black" and a little less "fainter than the faintest grey"
- RGB (1,1,1) is the faintest of greys, without bias

We see that between each pure grey level, there are six more nuanced intensities, but each expressing a primary bias. The six is actually 3! ("three factorial"), which is the number of unique [permutations](https://en.wikipedia.org/wiki/Permutation) three elements allow. With six primaries, we get 720 (6!) possible nuances.

In addition, because the human eye is better at distinguishing contrast in darks and less good at distinguishing contrasts in lights, the intensities are stored [after a logarithmic transform,](http://dcinema.me/2014/09/film-look-more-on-linear-log-gamma-curves), which you will probably know about due to [gamma correction](https://en.wikipedia.org/wiki/Gamma_correction) (although gamma correction is what you apply *afterwards*, to transform the logarithmically stored data back to a linear gradient on a screen) so that we use more bits to store darks, because they're more important, and fewer bits to store lights, because they're less important.

## So what happens when we add more primaries?

If we wanted to add more primaries, in order to capture more colours, we would:

- capture **vastly** more physical colours.
- increases the grey scale nuance,
- increases the colour bias per colour point,
- decreases the "wrongness" of that colour bias per colour point,
- increases the colour accuracy of the point,
- decrease the possible colour brightness, due to "inactive" color channels per quadrant (unless we make render devices that "mix in
 whites in order to deal with that. And that's not a problem you want to be payed to solve, it's pretty hard),
- may decrease the number of distinct points we can sample per square unit of surface,
- increases the number of *types* of sensors we need to do our sampling,
- does nothing for the size of the data that our recording device will end up creating.

# But we can do better

The title of this article is "We are really terrible at digital colours, *and digital photography*" and I think it's time to explain why the latter is the case.

In ye olden days, photography meant exposing a film that consisted of multiple layers of "reactive-to-certain-color-ranges" emulsion, to a focused light source, and then washing off all the stuff that didn't react. To then actually *see* the photograph, you'd either have to shine a bright light through the film (for ["positive"](https://en.wikipedia.org/wiki/Reversal_film) film), or capture the light as it was filtered by the film on photosensitive paper (for ["negative"](https://en.wikipedia.org/wiki/Negative_%28photography%29) film). Or, if you used a Polaroid, the paper WAS the film, and basically [magic happened](https://en.wikipedia.org/wiki/Instant_film).

Now, again, historical baggage has held us back: the first digital cameras really couldn't do much more than mimic this film process, with using a digital sensor: the shutter opens, light falls on the sensor, each point on the sensor "fills up" to some degree, the shutter closes, and then we ask each point "how much light did you collect?". This is exactly the same process as analog film, just done with digital sensors instead of reactive film emulsion.

But we're **still** doing that today, after decades of digital camera technology, and it really makes no sense anymore: why are we still lump-sum recording light instead of using sensors that can tell how much light passes through it per time unit, instead of "filling up"? What if instead of this:

- pixel 1: MAX light units after exposing for Y milliseconds (possibly blown out)
- pixel 2: MAX light units after exposing for Y milliseconds (possibly blown out)
- pixel 3: MAX light units after exposing for Y milliseconds (possibly blown out)
- pixel 4: 9900 light units after exposing for Y milliseconds
- pixel 5: 9700 light units after exposing for Y milliseconds
- ...

we can record this, instead:

- pixel 1: strong light-induced resistance Ω₁ => encode as MAX/1200 lumen/ms
- pixel 2: strong light-induced resistance Ω₂ => encode as MAX/1201 lumen/ms
- pixel 3: strong light-induced resistance Ω₃ => encode as MAX/1202 lumen/ms
- pixel 4: less strong light-induced resistance Ω₄ => encode as MAX/1305 lumen/ms
- pixel 5: even less strong light-induced resistance Ω₅ => encode as MAX/1510 lumen/ms
- ...

Instead of starting a sensor, letting light fall on it, and then turning it off and asking it how much light it found, we have the technological capability to make sensors that change [electrical resistance based on how strong a light hits them](https://en.wikipedia.org/wiki/Photoresistor). And **that** information is truly digital: instead of a single exposure's worth of values, we can capture the entire exposure function for all points. And having **those** values means that all the algorithms we use on a daily basis for digital photography go from "hard problems" to "stupidly simple":

- the need for [HDR](https://en.wikipedia.org/wiki/High-dynamic-range_imaging) using multiple exposures *disappears*, because we have the exposure functions themselves that we can work with. HDR is simply "which exposure transformation would you like to apply?"
- [overexposure](https://en.wikipedia.org/wiki/Exposure_%28photography%29) at the sensor-level becomes virtually impossible. You'd have to be pointing the camera at something truly incredibly bright.
- leveling photographs becomes trivial because we no longer have to "guess" which parts of an image had high or low lighting coefficients.
- standard exposure control becomes trivial, since this is simply a variable you plug into the exposure functions for each pixel.

## What would go into the "RAW" data, then?

Right now, cameras encode the "how much light did you collect over X amount of time", plus camera setting as meta data, as
```
{
  header,
  metadata,
  raw data: {I₁, I₂, I₃, I₄, I₅, ...}
}
```

Each colour channel is a (possibly compressed) 14 bit integer representing that point's recorded amount of light. The channels don't have "a colour", they're basically just greyscale sensors (implementing full spectrum sensors would be amazing, but is so complicated it'd be a post on its own), but the light that falls into them will have been filtered such that a particular sensor will have recorded the intensity of a particular (interval) of wavelength(s) of physical light, and the information on which sensor recorded which "colour" comes from knowing which colour pattern is used by which camera.

So, in the same manner we could instead record:

```
{
  header,
  metadata,
  raw data: {Ω₁, Ω₂, Ω₃, Ω₄, Ω₅, ...}
}
```
to describe the light-induced resistance per point, with the information about which channel has which colour, and what the value represents, encoded in the meta data.

## Wouldn't we be missing the base offsets, upon which the coefficients build?

If you're observant, you'll have noticed that each sensor is basically used to evaluate the function `f(t) = a*t + b`, where `f(t)` is the final amount of light gathered, `t` is the time interval, `b` is some base value and `a` is the lighting coefficient. Current digital cameras capture only `f(t)` (the final value), and the proposed scheme only encodes `a`, the light coefficient, so we're effectively treating `b` as if it is zero.

And that's true, but here's an important thing to bear in mind: these two approaches yield *exactly the same values* when using the same exposure values. It's not until you start messing with the exposure time after the fact that the missing `b` starts to matter, and we clearly already know how to deal with that (since Lightroom etc. come with exposure control that seems to work at least sort of passably). Of course, ultimately we want a sensor that can record both a base offset and the coefficient but even if we have a sensor that can't, we're still strictly improving the technology.

In fact, let's talk about that for moment, because it's worth discussing: why do we reject improvements that we feel aren't the perfect solution, instead of applauding it for keeping things, at worst, the same, but on average, improved?


## If this is so crazy good, why aren't we already doing this?

Pretty much "because money". Digital photography is basically a monopoly between a very low number of players, and even if you have a fantastic idea, you're not likely to succeed because you're making something people don't understand they need. Think about the following:

*Why would you pay for a camera that doesn't do what you've come to expect a camera to do, especially when the technology it relies on is still in its infancy?*

We —people with money who can afford cameras *and* care about the technology— are really very irrational, and extremely dumb when it comes to investing in a better future: we'll reject technological solutions just because they are going to, necessarily, be clunky at first. We make for absolutely terrible early adopters.

A great example of this is [Lytro](https://lytro.com/), who make a camera that records a (partial) [light field](https://en.wikipedia.org/wiki/Light-field_camera). You have (likely) no idea what that means, so: it means you don't get information about "light focused at this point" for each image pixel, but you get information about "the various rays of light that ended up at this point arrived there by traveling through the lens at the following angles".

That is amazing! By tracking the angle by which light travels through the camera the very *notion* of focus has been made obsolete: "focus" is now a thing you can play with **after** taking a photograph. **That's a game changer!!**

But, alas, Lytro of course had to spend lots of money on researching the technology and making it production ready, and in order to make back that money, their cameras are expensive, and their file format is closed source, so you need *their* software to work with "light field photographs", rather than being able to load them up in something like Lightroom or Aperture, with a plugin to control the front and rear focal plane distances. Would you pay for something that restricts your workflow that much? I wouldn't. I love the idea of Lytro, but like almost everyone else, I'm still waiting for them to release an affordable camera with a Lightroom plugin.

Would uptake be better if they'd priced the devices competitively, and hadn't locked down their file format? Sure. Would they make enough money to go **beyond** that initial investment in the technology and improve upon it, to the point where it can actually compete on quality with already existing digital cameras? Probably not.

They're on their second camera as I write this, which costs US$1200 and while able to generate an amazing kind of data file, yields data that can only generate images measuring 2450 by 1634 pixels, with a fixed lens that is 30-250mm equivalent, with a whopping 3.2 crop factor sensor. Now, if you know digital photography, you might be thinking  that those values mean something, because you're used to hearing them, and you might be thinking that an equivalent dSLR made by Nikon or Canon, price wise, gets you a camera that has a four times higher resolution, with better interchangeable lenses (30-250mm? *think of the distortion!*) , and a 1.5-1.2 crop factor.

Of course, 2450 by 1634 pixels is wider than what you're publishing to the internet, and of course that zoom range means nothing in terms of what you're used to from "interchangeable SLR lenses" since the lens has to work for light fields, not planar recording, and you've never worked with a lens that has to do that, so your knowledge of what is a "safe" zoom range for dSLR is absolutely irrelevant for a Lytro camera, but you're going to lie to yourself and pretend you know what you're talking about, and you won't be buying a Lytro camera, and that's the battle we're fighting:

*our own biased nature.*

Unless you're made of money, you're simply not going bother with a Lytro camera, because it does less, while costing more. But, if no one buys it, Lytro will probably lose out on money they need to improve the technology they have to the point where their cameras are on par with the equivalent Nikon or Canon. And if they start losing money, it's a good bet someone like Nikon or Canon will buy them. And then bury them. Because it doesn't fit into their money making scheme.

## So... the bottom line is...

These improvements aren't going to happen. At least not in any immediate future, and certainly not unless those improvements solve "shortcomings in photography". And, because we've been using dSLRs for so long now, no one really seems to think about how incredibly huge the shortcomings are, given where we could be if we tried a little harder.

The real problem is that this deficiency is not losing Nikon and Canon any money (and let's be honest: yes, there are other manufacturers and no: they are utterly irrelevant when it comes to moving the market). I could write the most amazing white paper, with all the tech specs worked out, the sensors described to the last pin, the control board attached as digital PCB file, the image codecs implemented in C, Java, Python and Nodejs-flavoured JavaScript, with a freely hosted fully functional, cross-platform demonstration editor that shows off everything that you can easily do now that was hard before, and we'd **still** get nowhere because all Nikon and Canon have to do is ignore it, and the idea will die all on its own. I don't have the funds to build the actual camera and compete in a market that doesn't realise how non-digital the technology is, nor how good the technology *could* be if they demanded the improvements.

The digital revolution brought many cool things when it happened, but the fallout is that hardware wise, we're still suffering from the decisions that were made based on conditions that haven't applied for years now, and the people with the power to change that don't care. As much as I love the new consumer circuitry revolution, with cheap [arduinos](https://www.arduino.cc) and the [raspberry pis](https://www.raspberrypi.org) and [adafruit](http://www.adafruit.com) at the forefront, these things are not enough to disrupt an industry at the chip level. Creating a new image sensor is not on the same level as writing a free Photoshop alternative, or a free quadcopter design - there are no DIY solutions for this problem. Hopefully "yet", but I kind of suspect "full stop". I can't even create a six-colour filter for fitting on top of an existing image sensor without RGB bayer filter because there just aren't any DIY ways to achieve that, and I'm not a student or retired engineer with enough spare time to make this work. Unless I quit my job. Which wouldn't be a very smart thing to do.

I can't make this real, no matter how much I want it to be: **sometimes, the future is not now. Sometimes, the future is far, far away.**

And that makes me sad.

And if you are now sufficiently angry because you feel "you can totally do this" then I hereby officially challenge you to go do it. I don't care who makes it happen: if you think you can make it happen, *make it happen*, and I will buy your camera. Because I want you to succeed.

## Addendum: answers to questions from readers

Some people had some questions about whether this was practical or theoretical, and —quite legitimately— wondering whether any of this actually matters. After all, traditional film didn't have full coverage either.

So, first off: it is entirely true that traditional film doesn't cover the full gamut either, but traditional film worked a little differently from digital sensors - their gamuts aren't "triangular", they're far more wobbly (as [Duncan Wilcox](https://twitter.com/duncanwilcox/status/620892975468572673) points out, for instance), and they're not all restricted to a single gamut; different films cover different colour ranges. So film lets us at least partially deal with the issue of bad color spaces: the wobbliness actually gives us far more nuance in areas where it matters. In the same way that digitizing audio by throwing away "the parts we can't really hear" is a great way to bring down signal complexity, it's important to throw away "the parts that don't matter" in the visual spectrum too, and the problem there is that the triangular gamuts throw away "too much". The nuances in green are lost in virtually all colour spaces, while traditional film preserves them. Certainly, they will come out looking a little different from film type to film type, but that's the other reasons why film at least had a solution: if you knew what you were shooting, you could pick the film to match that. While it was a hassle, this was a workable thing, allowing you to preserve the nuance you needed in those parts of the spectrum where it mattered (the digital equivalent would be to swap out image sensors based on the gamut you need, which isn't really a thing digital cameras can do). So film was certainly not perfect either, but it wasn't perfect in a slightly different way, and allowed users to work around the limitations to some extent.

Now, *technically* digital cameras also have "wobbly" sensors, but if you shoot in "not RAW", the camera will force the image data to fit the aRGB gamut, and even if you shoot RAW you're probably missing out on a fair bit of colour. The older [Nikon D300s](http://www.amateurphotographer.co.uk/reviews/dslrs/nikon-d300s/8), or newer [Canon EOS 7D](http://www.amateurphotographer.co.uk/reviews/dslrs/canon-eos-7d/8), for instance, can capture those rich purples in RAW mode, but physically cannot record a fair number of greens that even aRGB covers, so if that data has to be "compressed" back into aRGB for putting online (kind of what everyone is doing these days) then we don't just lose those rich colours, we also distort our greens.  That plain old sucks.

Then: the question "Is this problem academic?" and to that I have to categorically say "No, not even slightly".  Every time you pick up a digital camera and walk out to take a picture of blue or purple flowers, or some particularly inspiring mixture of green leaves, you'll end up disappointed that the digital result looks nothing like what you actually saw before you took the picture, simply because real life has tons of colours that simply don't fit in your camera's digital colour space. They get "capped off" at the edge of the gamut. Even a colour-calibrated workflow won't help here: I have four digital cameras and a high resolution scanner and two large monitors, all of which get colour-calibrated independently (the monitors with an ICC profile that gets rechecked every two weeks, the cameras by way of a colour calibration sheet that is shot along with subjects and then corrected for in Adobe Lightroom). However, even with colour calibration you cannot solve the problem of "trying to record colours that *don't fit* in the sensor's gamut" because the sensor literally cannot record them. Colours close to the edge will sort-of trigger the sensor and yield a faint signal of the wrong colour, and colours further from the edge simply don't trigger anything. They don't exist, as far as the sensor knows, in the same way UV and IR don't make it into your photographs (unless you bought a dedicated uv/ir camera, or hacked your sensor filter).

And it's not just nature: I've spent weeks trying to properly digitise sheets with some 500+ different fountain pen inks, in all the colours of the rainbow (and a few that aren't in it, I'm sure), trying differing calibrations and devices to try to get "at least most of the colours" to match. That process just grinds to a halt because of limitations in technology every time. You basically end up slicing up each photograph and then manually trying to correct each individual ink to match what's on paper, which isn't even possible for a fair number of colours because they're gorgeous, and absolutely nowhere inside the aRGB gamut. So, unfortunately, this is a real problem that I run into so often that the built up frustration over it culminated in the article you just read.

Finally, to those people who feel I need an editor: *I absolutely agree*, which is why I generally try to reread my articles a few days after posting so I'm reading it with a fresh eye. Until then, if you spot horrible typos or weird phrases, my blog *including the posts* are open source, so [have a look at the source for this post](https://github.com/Pomax/pomax.github.io/blob/master/gh-weblog/content/posts/markdown/2015-07-14-01-12-40.md) and rewrite it for me if you think that'll improve the phrasing!