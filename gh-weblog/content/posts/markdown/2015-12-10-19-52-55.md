In a previous post [I talked about the distinction between "TTF" and "OTF" fonts](/1449438115186), and the fact that they're actually both just OpenType fonts. I explained how the difference between TTF and OTF is only which outline language is used, with everything else being the same between the two types of Opentype font, so in this post I'd like to talk about some of the bits that all OpenType fonts have. There are actually quite a lot of things, so this will not be the last post on the subject, but let's just cover as much as we can and see what that leaves.

Let's start with something that might sound like it can't possibly be true:

## Fonts don't contain letters.

I know this sounds a bit weird for a technology that is supposed to be used to draw letters, but it's important to get this out of the way first: fonts don't know anything about "letters". Instead, they know about character codes, and glyphs. Character codes are simply numbers that the computer sees when you type, when it reads a text file, etc. and glyphs are anything in the font "that can be drawn". A font is a collection of inherently meaningless pictures, linked to character codes to give them meaning.

### The problem with defining "letters"

The reason fonts contain character codes and glyphs, rather than "letters" is that letters are a very restricted class of things we write. For instance, clearly the "letters" a through z are letters; this is true by definition. But are numbers letters? We can say that Fonts contain numbers and letters, but then what about symbols? Okay, fonts contain numbers, letters, and symbols. Clearly we're done. But what about CJK languages? Their writing systems don't use "letters": Korean has syllables, instead, Chinese has logograms, and Japanese uses both. So now things are getting very tricky. But let's make it even more fun: let's look at different kinds of the same letter. In English, the 'a' and the 'A' are both the same letter, as well as different letters. If you had to group the elements of a set "a,a,A,a,a,A,A,b,A,b,b,a,B,B,B", you would probably group them as a's in one pile, and b's in another. But if you had to group a full mixture of the alphabet in upper and lowercase, you'd probably sort them according to case. Are they the same letter or different letters? What about Arabic, in which each letter has four possible shapes depending on where in a word it's used (a letter will look different depending on whether it's on its own, or at the start of, middle of, or end of a word). What about those, are those all different letters, or the same one?

So in order to get around this problem, fonts don't contain letters. They contain [glyphs](https://en.wikipedia.org/wiki/Glyph): anything that you can draw an outline for can be made a glyph, so that in a font: yes, all those different forms of the same thing are different things, and it's up to the character mapping and substitution rules specified in the Opentype font to determine which glyph should be used when.


### Character code sets

So where do the character mappings come from? How many are there? Which are "the best"?

For convenience, I'm going to ignore a *lot* of history, mostly because you can write full chapters on the history of character mappings, and we'd still not get to where I want to get in this post, so: lots of different people and organisations have coined lots of different character mappings throughout the history of computing, and of those, I want to focus on two major ones: ASCII, and Unicode.

[ASCII](https://en.wikipedia.org/wiki/ASCII) is probably the oldest, still widely used, standard, and it was invented to capture the symbols used in American computing, using 7 bits. Because back in the day, there weren't necessarily 8 bits to a byte, sometimes you have 7, or 6, or some other value that we now think of as weirdly "not eight". Anyway, ASCII uses 7 bits, and so there are 128 possible codes available: 0 through 127. The first 32 codes are "control" codes, not so much related to typing things as controlling the computer: escape, backspace, tabulation, line feed, data acknowledgement, things that control what the computer will do. After that are the numbers, letters, and symbols that were commonly (and a few uncommonly) used in American computing environments, and the very last code is the "delete" code.

Of course, at the same time the rest of the world was inventing tons of other encodings because America wasn't the only country in the world using computers, and as the world became more and more connected, the more we had problems around trying to read text written by someone who wasn't using the same encoding, as well as needing way more than just 128 "spots" to capture all the different things necessary for each language, let alone "all languages".

And that's where [Unicode](https://en.wikipedia.org/wiki/Unicode) comes in: Unicode was, and remains, our best effort to fit all writing systems in this world into a single character mapping. As you can imagine, that mapping is huge: over 120,000 codes at this point in history (in June 2015, the Unicode standard v8 was released). But: the standard is public, it's stable (meaning that it's never going to fix historical mistakes like two different codes that point to the same character, but that no one noticed before accepting the code into the standard), and it takes a fairly long time to get new codes assigned specifically to make sure that standard doesn't "just grow" as we add more and more languages into it, but actually has some internal logic that computer scientists, programmers, etc. can rely on.

The one thing that made switching to Unicode even possible, was that the first 128 codes in Unicode are, in fact, the ASCII codes, which made it possible to switch from ASCII to Unicode with virtually no effort. ASCII data, in an 8 bit world, is automatically data that matches the Unicode mappings, "encoded using the UTF-8 scheme".

### Byte encodings

If you have a standard that contains way more numbers than fit in 8 bits, you need an extra step to turn possibly huge numbers into sequences of bits. Where ASCII is both the character code set and the encoding, Unicode needs a bit more work: it is only the character code set, and in order to write those codes as bytes on a disk, it has various "byte encoding" schemes available. All of these encode "Unicode data", but byte patterns on the disk can look radically different.

The two common encodings for Unicode are things you've probably heard of, even if you don't know what they are exactly: UTF-8, or UTF-16. For details on both, wikipedia is a great source of information, so I will just summarise them here.

[UTF-16](https://en.wikipedia.org/wiki/UTF-16) simply "tried" to capture every code using 16 bits, and then when there were so many codes that it became obvious that 16 bits would not be enough, a special few codes were reserved for "if you see this code, it's actually not a character mapping itself, but has to be combined with the *next* 16 bits to form one of the higher numbers that don't fit in 16 bits".

From this description, you'd imagine that [UTF-8](https://en.wikipedia.org/wiki/UTF-8) does something similar, and it totally does, except it's generally far more efficient than UTF-16 in how it does it; UTF-8 is a variable byte encoding, with marker bits to say how many bytes are used, and whether a byte is a "start" byte or a follow-up byte. It matches ASCII byte-for-byte by starting with the bit 0 ("there are no next bytes") and then the 7 bit ASCII value. For any value larger than 128, needing more than 1 byte, the first byte starts as 1,1,0 if there are going to be 2 bytes used, 1,1,1,0 for three, 1,1,1,1,0 for four, etc. Any subsequent byte starts with 1,0 (so if you pick a random spot in a UTF-8 stream, and you see a byte that starts as 1,0 you know you need to move forward or backward a little so you're not in the middle of a character's code representation).


### from code sets to glyphs

So on the conceptual side, we have "things you can write", which we capture as glyphs in a font, and a character mapping that assigns each of those things a number, but then we still have a problem: fonts don't have to implement every possible character, so if we look at the character mappings that are actually *used* in a font, there might be lots of disjointed sets of supported, and not supported mappings.

Fonts, on the other hand, are relatively efficient in how they store glyphs: they use arrays. The first glyph is in spot 0, the second glyph is in spot 1, the third in spot 2, and so forth: they are a stored in a list without gaps, and their ordering in that list is not related to any encoding or character mapping. It's just a list of pictures with ids derived directly from their position in the list.

So even if we have our list of glyphs, and we know which character mapping our font will work with, and we know which parts of that mapping the font will support, we still need one more step to make sure we can look up with character code maps to which position in the font's list of glyphs.


### "internal" glyphs

Finally, there can also be glyphs that don't map to character codes. This sounds a bit odd, because how would you ever see them, but glyphs don't need to specify all the outline data in a giant wad of instructions: they can also contain references to other glyphs. For instance, the Japanese character 松 (pine tree) and 柿 (persimmon) have the same "drawing" on the left side. We can store that particular outline as an internal glyph, with its own glyph id, and then form those two characters by starting each off as saying "place the glyph at id ... on the left" before explicitly specifying the outlines for the rest of the glyph.

As such, a font kind of *can* contain gaps in the list of glyphs, not by having "nothing" in the list in some spots, but by having glyphs that don't have a mapping to any character code. Things can get really tricky!

## The CMAP Table


At this point you might think: "...but there are so many encodings and character mappings and no rules on how to order glyphs in that list, there must be millions of ways for that final mapping to look!" And you're not wrong. There really are an incredible number of "character code to font-internal glyph id" mappings that could exist, and so OpenType comes with not just one, but several different ways to set up a "character to glyph id" mapping, based on a few things like "does the font implement one, or multiple, sets of character codes", "can there be gaps in the sets or not", "are any of the sets not commonly implemented sets", etc. By answering these questions, you can find the best charcode to id mapping for what your font will support, and then based on that you can follow a small number of rules around how to order the glyphs in the list of glyphs so that everything will work efficiently.

The thing that makes this work in OpenType fonts is called the [`CMAP`](https://www.microsoft.com/typography/otspec/cmap.htm) table, and if you're still thinking "wait, that sounds like still not enough to capture all those possibilities", you're still right, and OpenType lets you specify *multiple* CMAP subtables, which can all be different, to efficiently cover the entire range of supported characters in your font.

While you might think that the pictures inside the font are the most important part of a font, in terms of how fonts *work*, the CMAP is the absolutely most important part: if the outlines are missing, but we have a CMAP, at least the CMAP can tell us there are no pictures to work with, and if any of the OpenType metadata is missing, we might be rendering the text in a really weird way, but without a CMAP we don't even know what characters a font supports, or how to even get to any of the pictures we need to render text. Without a CMAP, a font is just a useless binary file.

(And yes, historically there have been font formats that actually had outline data in one file, being the 'useless binary file', and a separate cmap file that you could load to give meaning to the 'useless data')


## Substitutions

Finally, we need one more thing to deal with issues around "which glyph do we even use?" such as what we saw for Arabic: one letter, but depending on where in a word it is, it has to look different. Or closer to English: ligatures, where typing an 'f' followed by an 'i' tends to generate some shape fi that looks different, and is in fact a different glyph, from the separate letters.

This is called "substitution", and is handled by a table called the [`GSUB`](https://www.microsoft.com/typography/otspec/gsub.htm) table. It allows various kinds of substitutions, with rules that can be different depending on the language or script the font has to style: substitution rules for English may not need to apply in Vietnamese, for instance. In order to deal with all kinds of substitutions, the GSUB table is [split up into several sections](https://www.microsoft.com/typography/otspec/chapter2.htm):

- scripts, which say which "language" the text is in
- features, which control when substitutions kick in (such as common ligatures, historical number forms, etc)
- lookups, which are the actual "A becomes B" rules.

To get a substitution set up, you need a lookup (to say which actual substitution has to happen), tied to a feature (which gives the typographical context for the lookup), tied to a script (which gives the linguistic context for the feature). All of these are many-to-many, so one feature could be used by multiple scripts, and one script can use multiple features (and then the same for features and lookup relations).

Substitution lookups also come in various forms, so that there are special structions for one-for-one substitutions (like turning every full stop into a Japanese sentence end marker), many-for-one (like word to icon substitution), initial, medial, and final substitutions (for languages like Arabic) and the rather complicated "contextual substitution" which lets you define patterns in the text that have to be true for the substitution to kick in.

## Wrapping up

While it's tempting to argue about TTF vs OTF, the thing that really drives a font is the CMAP. Hopefully you now understand the core concept for modern OpenType fonts a bit better, understanding how a font knows which characters it supports, and how it it maps what you type to what you see.

In a next post I'll cover some more typographical concepts like kerning and positioning, as well as some of the finer nuances between TrueType and Type2 outlines, which work together with the pure OpenType data about those things.

And if you have questions, or comments, do let me know by clicking the comment link below.